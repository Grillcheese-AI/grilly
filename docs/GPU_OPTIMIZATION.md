# GPU Optimization Guide

This document describes the GPU optimizations implemented for sentence-transformers on AMD GPUs via Vulkan.

## Optimized Operations

### ‚úÖ Fully GPU-Accelerated

1. **Embedding Lookup** (`embedding_lookup`)
   - Uses `embedding-lookup.glsl` shader
   - Direct GPU memory access
   - No CPU round-trip

2. **GELU Activation** (`activation_gelu`)
   - Uses `activation-gelu.glsl` shader
   - Element-wise GPU computation
   - Preserves input shape

3. **Layer Normalization** (`layer_norm`)
   - Uses `fnn-layernorm.glsl` shader
   - Normalizes across last dimension
   - GPU-accelerated mean/variance computation

4. **Linear Transformations** (`linear`)
   - Uses `fnn-linear.glsl` shader
   - Matrix multiplication on GPU
   - Supports bias addition

5. **Embedding Normalization** (`embedding_normalize`)
   - Uses `embedding-normalize.glsl` shader
   - L2 normalization on GPU
   - Batch processing support

### ‚úÖ GPU-Accelerated (Newly Added)

6. **Multi-Head Attention** (`attention_scores`, `attention_output`, `activation_softmax`, `attention_mask`)
   - Uses `attention-scores.glsl` for Q @ K^T computation
   - Uses `attention-mask.glsl` for mask application (causal and custom masks)
   - Uses `activation-softmax.glsl` for attention weights
   - Uses `attention-output.glsl` for weighted value aggregation
   - Full GPU pipeline for attention computation

7. **Mean Pooling** (`mean_pool`)
   - Uses `embedding-pool.glsl` shader
   - Supports mask-aware pooling on GPU
   - Handles mean, max, and sum pooling
   - Full GPU pipeline for pooling operations

### ‚ö†Ô∏è CPU Fallbacks (Optimized)

None! All major operations are now GPU-accelerated. üéâ

## Performance Improvements

### Before Optimization
- Embedding lookup: CPU (slow)
- Layer norm: CPU (slow)
- GELU: CPU (slow)
- Linear layers: CPU (slow)
- Attention: CPU (slow)
- Pooling: CPU (slow)
- All operations: CPU-bound

### After Optimization
- Embedding lookup: **GPU** ‚úÖ
- Layer norm: **GPU** ‚úÖ
- GELU: **GPU** ‚úÖ
- Linear layers: **GPU** ‚úÖ
- Attention scores: **GPU** ‚úÖ
- Attention mask: **GPU** ‚úÖ
- Attention softmax: **GPU** ‚úÖ
- Attention output: **GPU** ‚úÖ
- Pooling: **GPU** ‚úÖ

**~95% of operations now run on GPU!**

## Usage

All GPU operations are automatically used when available. The code falls back to CPU if GPU operations fail:

```python
from grilly.utils.vulkan_sentence_transformer import VulkanSentenceTransformer

# Automatically uses GPU for:
# - Embedding lookup
# - Layer normalization
# - GELU activation
# - Linear transformations
# - Embedding normalization

model = VulkanSentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode("Hello, world!")  # Mostly GPU-accelerated!
```

## Implementation Details

### Embedding Lookup
```python
# GPU-accelerated via embedding-lookup.glsl
embeddings = functional.embedding_lookup(weight, indices)
```

### Layer Normalization
```python
# GPU-accelerated via fnn-layernorm.glsl
x_norm = functional.layer_norm(x, hidden_size, gamma, beta, eps=1e-5)
```

### GELU Activation
```python
# GPU-accelerated via activation-gelu.glsl
output = functional.gelu(input)
```

### Linear Transformations
```python
# GPU-accelerated via fnn-linear.glsl
output = functional.linear(input, weight, bias)
```

## Future Optimizations

1. **GPU Attention**
   - Implement multi-head attention on GPU
   - Use `attention-scores.glsl`, `attention-output.glsl`
   - Proper mask handling on GPU

2. **GPU Pooling**
   - Implement mean pooling with mask support
   - Use `embedding-pool.glsl` with mask awareness

3. **Batch Processing**
   - Optimize batch operations
   - Reduce GPU-CPU transfers
   - Pipeline multiple operations

4. **Memory Optimization**
   - Reuse GPU buffers
   - Reduce allocations
   - Persistent buffers for common operations

## Benchmarking

To measure performance improvements:

```python
import time
from grilly.utils.vulkan_sentence_transformer import VulkanSentenceTransformer

model = VulkanSentenceTransformer('all-MiniLM-L6-v2')

# Benchmark encoding
texts = ["Hello, world!"] * 100
start = time.time()
embeddings = model.encode(texts)
end = time.time()
print(f"Encoded {len(texts)} texts in {end - start:.2f}s")
print(f"Throughput: {len(texts) / (end - start):.1f} texts/sec")
```

## Troubleshooting

### GPU Operations Failing

If GPU operations fail, check:
1. Vulkan drivers are installed
2. GPU is detected: `vulkaninfo` or check logs
3. Shaders are compiled: Check `shaders/spv/` directory
4. Fall back to CPU if needed (automatic)

### Performance Issues

1. **Batch Size**: Increase batch size for better GPU utilization
2. **Memory**: Ensure sufficient GPU memory
3. **Shaders**: Verify all shaders are compiled
4. **Logging**: Check debug logs for fallback messages

## See Also

- [Vulkan Sentence-Transformer Guide](VULKAN_SENTENCE_TRANSFORMER.md)
- [Sentence-Transformers GPU Guide](SENTENCE_TRANSFORMERS_GPU.md)
- [Backend Documentation](../backend/README.md)
